import timeit

import torch as t
import torch.nn as nn
import torch.nn.functional as F
import scipy.sparse as sp
import numpy as np

class Model(nn.Module):

    # parameter:ä¸€ä¸ªç”¨æ¥å­˜å‚¨æ‰€ç”¨å®ä½“çš„å­—å…¸ï¼ˆuserNumã€itemNumç­‰ç­‰ï¼‰
    # args:ç”¨æ¥è°ƒç”¨parserçš„å‚æ•°ï¼Œæ–¹ä¾¿åœ¨å‘½ä»¤è¡Œä¿®æ”¹

    def __init__(self, config, args, device):
        super(Model, self).__init__()
        ...
        # åˆå§‹åŒ–å€¼
        self.device = device

        self.folds = 10
        self.ssl_temp = 0.1
        self.dropout = False

        self.A_split = args.A_split
        # keep_prob å‚æ•°è¡¨ç¤ºä¿ç•™ç¥ç»å…ƒçš„æ¦‚ç‡ï¼Œå…¶å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªè¾ƒå°çš„å€¼ã€‚
        self.keep_prob = args.keep_prob
        self.n_layers = args.n_layers
        self.hide_dim = args.hide_dim
        hide_dim = self.hide_dim
        self.alpha = args.alpha
        self.n_factors = args.n_factors
        self.batch_size = args.batch_size
        self.regs = eval(args.regs)
        self.decay = self.regs[0]
        self.emb_dim = args.embed_size
        self.LayerNums = args.LayerNums
        # ç‰¹å¾èåˆç³»æ•°
        self.wu1 = args.wu1
        self.wu2 = args.wu2
        self.wi1 = args.wi1
        self.wi2 = args.wi2
        # èåˆç³»æ•°
        self.ssl_ureg = args.ssl_ureg
        self.ssl_ireg = args.ssl_ireg
        self.ssl_temp_h = args.ssl_temp_h

        self.n_users_1 = config['userNum_S']
        self.n_users_2 = config['userNum_T']
        self.n_items_1 = config['itemNum_S']
        self.n_items_2 = config['itemNum_T']
        self.norm_adj_1 = config['adj_mat_S']
        self.norm_adj_2 = config['adj_mat_T']
        self.uuMat_1 = config['u2u_mat_S']
        self.uuMat_2 = config['u2u_mat_T']
        self.iiMat_1 = config['i2i_mat_S']
        self.iiMat_2 = config['i2i_mat_T']
        self.uiMat_1 = config['ui_mat_S']
        self.uiMat_2 = config['ui_mat_T']


        # GCN ç¼–ç å±‚çš„åˆå§‹åŒ–
        self.encoder = nn.ModuleList()  # å®ƒç”¨äºå­˜å‚¨ä¸€ç³»åˆ—å›¾å·ç§¯å±‚
        for i in range(0, self.LayerNums):
            self.encoder.append(GCN_layer())  # å¾ªç¯æ·»åŠ äº†å¤šä¸ª GCN_layer åˆ°æ¨¡å‹ä¸­ã€‚è¿™äº›å±‚å°†ç”¨äºå›¾æ•°æ®çš„ç‰¹å¾ç¼–ç ã€‚

        # ä¸¤ä¸ªçº¿æ€§å˜æ¢å±‚åˆå§‹åŒ–ï¼Œè¿™äº›å±‚ç”¨äºæ‰§è¡Œä»å¤šä¸ªç‰¹å¾åˆ°éšè—è¡¨ç¤ºçš„çº¿æ€§å˜æ¢
        self.meta_netu = nn.Linear(self.hide_dim * 3, self.hide_dim, bias=True)
        self.meta_neti = nn.Linear(self.hide_dim * 3, self.hide_dim, bias=True)

        self.k = args.rank  # ä½ç§©çŸ©é˜µåˆ†è§£çš„ç»´åº¦
        k = self.k

        # MLP å±‚çš„åˆå§‹åŒ–
        # å››ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¨¡å‹ï¼Œæ¯ä¸ª MLP æ¨¡å‹ç”±ä¸¤ä¸ªéšè—å±‚ç»„æˆã€‚è¿™äº›æ¨¡å‹å°†ç”¨äºå­¦ä¹ ç”¨æˆ·å’Œç‰©å“çš„åµŒå…¥è¡¨ç¤ºã€‚
        self.mlp = MLP(hide_dim, hide_dim * k, hide_dim // 3, hide_dim * k)
        self.mlp1 = MLP(hide_dim, hide_dim * k, hide_dim // 3, hide_dim * k)
        self.mlp2 = MLP(hide_dim, hide_dim * k, hide_dim // 3, hide_dim * k)
        self.mlp3 = MLP(hide_dim, hide_dim * k, hide_dim // 3, hide_dim * k)

        # ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹çš„åˆå§‹åŒ–éƒ¨åˆ†ï¼Œç”¨äºåˆå§‹åŒ–ç”¨æˆ·å’Œç‰©å“çš„åµŒå…¥ï¼ˆembeddingï¼‰å‚æ•°
        self.embedding_dict = nn.ParameterDict({
            'user_embedding_S': nn.Parameter(nn.init.normal_(t.empty(self.n_users_1, self.emb_dim), std=0.1)),
            'user_embedding_T': nn.Parameter(nn.init.normal_(t.empty(self.n_users_2, self.emb_dim), std=0.1)),
            'item_embedding_S': nn.Parameter(nn.init.normal_(t.empty(self.n_items_1, self.emb_dim), std=0.1)),
            'item_embedding_T': nn.Parameter(nn.init.normal_(t.empty(self.n_items_2, self.emb_dim), std=0.1))
        })

        # é¦–å…ˆï¼Œæˆ‘ä»¬åˆ†é…id-å¯¹åº”çš„åµŒå…¥eğ‘¢,ei
        self.gating_weightub = nn.Parameter(t.FloatTensor(1, hide_dim))  # æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„æ¨¡å‹å‚æ•°ï¼Œå®ƒæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (1, hide_dim) çš„æƒé‡çŸ©é˜µ
        nn.init.xavier_normal_(self.gating_weightub.data)  # å°†å…¶åˆå§‹åŒ–ä¸º Xavier åˆå§‹åŒ–çš„éšæœºå€¼
        self.gating_weightu = nn.Parameter(t.FloatTensor(hide_dim, hide_dim))
        nn.init.xavier_normal_(self.gating_weightu.data)
        self.gating_weightib = nn.Parameter(t.FloatTensor(1, hide_dim))
        nn.init.xavier_normal_(self.gating_weightib.data)
        self.gating_weighti = nn.Parameter(t.FloatTensor(hide_dim, hide_dim))
        nn.init.xavier_normal_(self.gating_weighti.data)

        # åˆ›å»ºä¸€ä¸ª Sigmoid æ¿€æ´»å‡½æ•°å®ä¾‹ï¼Œå®ƒå°†åœ¨æ¨¡å‹çš„åç»­è®¡ç®—ä¸­ä½¿ç”¨ã€‚
        self.f = nn.Sigmoid()
        # åˆ›å»ºä¸€ä¸ªçº¿æ€§å±‚å®ä¾‹ï¼Œè¿™ä¸ªçº¿æ€§å±‚çš„è¾“å…¥å’Œè¾“å‡ºç»´åº¦éƒ½æ˜¯ self.emb_dimã€‚åœ¨æ¨¡å‹çš„åç»­è®¡ç®—ä¸­ï¼Œçº¿æ€§å±‚ä¼šå°†è¾“å…¥æ•°æ®è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå¹¶è¾“å‡ºåˆ°ä¸‹ä¸€å±‚ã€‚
        self.linear = nn.Linear(self.emb_dim, self.emb_dim)
        # è°ƒç”¨æ¨¡å‹å†…éƒ¨çš„ _init_graph æ–¹æ³•ï¼Œç”¨äºåˆå§‹åŒ–å›¾ç»“æ„ã€‚è¿™éƒ¨åˆ†ä»£ç å¾ˆå¯èƒ½æ¶‰åŠåˆ°é‚»æ¥çŸ©é˜µçš„æ“ä½œå’Œåˆå§‹åŒ–ã€‚
        self._init_graph()

    # å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºç¨€ç–å¼ é‡
    def _convert_sp_mat_to_sp_tensor(self, X):
        coo = X.tocoo().astype(np.float32)
        row = t.Tensor(coo.row).long().to(self.device)
        col = t.Tensor(coo.col).long().to(self.device)
        index = t.stack([row, col])
        data = t.FloatTensor(coo.data).to(self.device)
        return t.sparse.FloatTensor(index, data, t.Size(coo.shape)).to(self.device)

    # å°†çŸ©é˜µAæŒ‰ç…§æŒ‡å®šçš„ä»½æ•°ï¼ˆself.foldsï¼‰è¿›è¡Œåˆ‡å‰²ï¼Œæ¯ä»½çš„é•¿åº¦ç”±n_userså’Œn_itemså†³å®šã€‚
    # åˆ‡å‰²åçš„å­çŸ©é˜µè¢«è½¬æ¢ä¸ºç¨€ç–å¼ é‡ï¼Œå¹¶å­˜å‚¨åœ¨A_foldåˆ—è¡¨ä¸­è¿”å›
    def _split_A_hat(self, A, n_users, n_items):
        A_fold = []
        fold_len = (n_users + n_items) // self.folds
        for i_fold in range(self.folds):
            start = i_fold * fold_len
            if i_fold == self.folds - 1:
                end = n_users + n_items
            else:
                end = (i_fold + 1) * fold_len
            A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(self.device))
        return A_fold

    # ç”¨äºåˆå§‹åŒ–å›¾ç»“æ„ã€‚è¿™éƒ¨åˆ†ä»£ç å¾ˆå¯èƒ½æ¶‰åŠåˆ°é‚»æ¥çŸ©é˜µçš„æ“ä½œå’Œåˆå§‹åŒ–ã€‚
    def _init_graph(self):
        if self.A_split:
            self.Graph_1 = self._split_A_hat(self.norm_adj_1, self.n_users_1, self.n_items_1, self.device)
            self.Graph_2 = self._split_A_hat(self.norm_adj_2, self.n_users_2, self.n_items_2, self.device)
        else:
            self.Graph_1 = self._convert_sp_mat_to_sp_tensor(self.norm_adj_1)
            self.Graph_1 = self.Graph_1.coalesce().to(self.device)
            self.Graph_2 = self._convert_sp_mat_to_sp_tensor(self.norm_adj_2)
            self.Graph_2 = self.Graph_2.coalesce().to(self.device)

    def __dropout_x(self, x, keep_prob):
        size = x.size()
        index = x.indices().t()
        values = x.values()
        random_index = t.rand(len(values)) + keep_prob
        random_index = random_index.int().bool()
        index = index[random_index]
        values = values[random_index]/keep_prob
        g = t.sparse.FloatTensor(index.t(), values, size)
        return g

    def __dropout(self, keep_prob, ori_graph):
        if self.A_split:
            graph = []
            for g in ori_graph:
                graph.append(self.__dropout_x(g, keep_prob))
        else:
            graph = self.__dropout_x(ori_graph, keep_prob)
        return graph

    # è¯¥å‡½æ•°è®¡ç®—è·¨åŸŸæ¨èæ¨¡å‹çš„åµŒå…¥å‘é‡    å›¾æ•°æ®ã€ç”¨æˆ·åµŒå…¥ã€ç‰©å“åµŒå…¥ã€ç”¨æˆ·æ•°é‡ã€ç‰©å“æ•°é‡
    def computer(self, graph, users_emb, items_emb, n_users, n_items):
        all_emb = t.cat([users_emb, items_emb])  # å°†åŸŸä¸­çš„ç”¨æˆ·å’Œç‰©å“åµŒå…¥å‘é‡åˆå¹¶æˆä¸€ä¸ªå¤§çš„å‘é‡ all_embï¼Œå¹¶å°†å…¶ä½œä¸ºåˆ—è¡¨ embs çš„åˆå§‹å…ƒç´ ã€‚
        embs = [all_emb]

        if self.dropout:
            if self.training:
                print("droping")
                # åœ¨ä»£ç ä¸­ï¼Œå½“æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼æ—¶ï¼Œå¦‚æœå¯ç”¨äº† Dropoutï¼ˆself.dropout ä¸ºçœŸï¼‰ï¼Œå°±ä¼šä½¿ç”¨ keep_prob å‚æ•°æ¥æ§åˆ¶ç¥ç»å…ƒçš„ä¿ç•™æ¯”ä¾‹ï¼Œä»è€Œæ‰§è¡Œ Dropout æ“ä½œã€‚
                # å½“æ¨¡å‹ä¸å¤„äºè®­ç»ƒæ¨¡å¼æ—¶ï¼Œå°±ä¸ä¼šåº”ç”¨ Dropoutï¼Œå³ g_droped å°†ç›´æ¥ä½¿ç”¨åŸå§‹çš„å›¾æ•°æ®ã€‚
                g_droped = self.__dropout(self.keep_prob, graph)
            else:
                g_droped = graph
        else:
            g_droped = graph

        # åœ¨æ¨¡å‹çš„æ¯ä¸€å±‚ä¸­è¿›è¡Œå›¾åµŒå…¥æ“ä½œï¼Œç”Ÿæˆå¤šå±‚çš„åµŒå…¥ç»“æœï¼Œå¹¶å°†è¿™äº›ç»“æœå­˜å‚¨åœ¨ layer_embs åˆ—è¡¨ä¸­
        layer_embs = []  # å­˜å‚¨æ¯å±‚çš„åµŒå…¥ç»“æœ
        # n_factorsæ˜¯ä¸€ä¸ªæŒ‡å®šçš„å› å­æ•°é‡ï¼Œn_layers==3 æ˜¯ç¥ç»ç½‘ç»œçš„å±‚æ•°
        factor_num = [self.n_factors for i in range(self.n_layers)]  # ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†æ¯å±‚çš„å› å­æ•°é‡
        for layer in range(self.n_layers):
            n_factors_l = factor_num[layer]  # ä»£è¡¨å½“å‰å±‚çš„å› å­æ•°é‡
            all_embs_tp = t.split(all_emb, int(self.emb_dim / n_factors_l), 1)  # å°† all_emb æ²¿ç€ç»´åº¦ 1ï¼ˆåˆ—ç»´åº¦ï¼‰åˆ†å‰²æˆå¤šä¸ªå¼ é‡
            all_embs = []  # ç”¨äºå­˜å‚¨å½“å‰å±‚ä¸­çš„ä¸åŒå› å­çš„åµŒå…¥ç»“æœ
            for i in range(n_factors_l):  # ç”¨äºè¿­ä»£å½“å‰å±‚ä¸­çš„æ¯ä¸ªå› å­
                if self.A_split:
                    temp_emb = []
                    for f in range(len(g_droped)):
                        temp_emb.append(t.sparse.mm(g_droped[f], all_embs_tp[i]))
                    side_emb = t.cat(temp_emb, dim=0)  # è¿™ä¸ªå¼ é‡åº”è¯¥åŒ…å«äº†å½“å‰å±‚ä¸­å½“å‰å› å­çš„æ‰€æœ‰åµŒå…¥ä¿¡æ¯
                    all_embs.append(side_emb)
                else:
                    all_embs.append(t.sparse.mm(g_droped, all_embs_tp[i]))
            layer_embs.append(all_embs)  # å°†å½“å‰å±‚çš„åµŒå…¥ç»“æœåˆ—è¡¨ `all_embs` æ·»åŠ åˆ° `layer_embs` åˆ—è¡¨ä¸­ã€‚  # ä¸ºäº†è·Ÿè¸ªæ¯ä¸€å±‚çš„åµŒå…¥ä¿¡æ¯
            factor_embedding = t.cat([all_embs[0], all_embs[1]], dim=1)  # å°†å½“å‰å±‚ä¸­çš„ä¸åŒå› å­çš„åµŒå…¥ä¿¡æ¯æ‹¼æ¥åœ¨ä¸€èµ·
            embs.append(factor_embedding)
            all_emb = factor_embedding
        # å°†åµŒå…¥ç»“æœåˆ—è¡¨ embs ä¸­çš„æ‰€æœ‰å¼ é‡æŒ‰ç…§ç»´åº¦ 1 è¿›è¡Œå †å ï¼Œå¾—åˆ°ä¸€ä¸ªä¸‰ç»´çš„å¼ é‡ï¼Œå…¶ä¸­ç»´åº¦ 0 è¡¨ç¤ºæ ·æœ¬æ•°é‡ï¼Œç»´åº¦ 1 è¡¨ç¤ºå›¾å±‚æ•°ï¼Œç»´åº¦ 2 è¡¨ç¤ºåµŒå…¥çš„ç»´åº¦ã€‚
        embs = t.stack(embs, dim=1)
        # åœ¨ç»´åº¦ 1 ä¸Šè®¡ç®—æ‰€æœ‰å›¾å±‚åµŒå…¥çš„å¹³å‡å€¼ï¼Œå¾—åˆ°å¹³å‡åµŒå…¥ç»“æœ light_outã€‚è¿™å¯ä»¥çœ‹ä½œæ˜¯å¯¹ä¸åŒå›¾å±‚åµŒå…¥çš„æ±‡æ€»ã€‚
        light_out = t.mean(embs, dim=1)
        # ä½¿ç”¨ torch.split å‡½æ•°å°†å¹³å‡åµŒå…¥ light_out åˆ†å‰²æˆç”¨æˆ·åµŒå…¥å‘é‡ users å’Œç‰©å“åµŒå…¥å‘é‡ itemsï¼Œåˆ†å‰²ç‚¹ä¸º [n_users, n_items]ã€‚
        users, items = t.split(light_out, [n_users, n_items])
        # å‡½æ•°è¿”å›ç”¨æˆ·åµŒå…¥ usersã€ç‰©å“åµŒå…¥ itemsï¼Œä»¥åŠåµŒå…¥ç»“æœåˆ—è¡¨ layer_embs çš„æœ€åä¸€é¡¹ï¼Œè¡¨ç¤ºæœ€åä¸€å±‚çš„åµŒå…¥ç»“æœã€‚
        return users, items, layer_embs[-1]

    # è‡ªä¸»é—¨æ§æœºåˆ¶
    def self_gatingu(self, em):
        return t.multiply(em, t.sigmoid(t.matmul(em, self.gating_weightu) + self.gating_weightub))
    def self_gatingi(self,em):
        return t.multiply(em, t.sigmoid(t.matmul(em,self.gating_weighti) + self.gating_weightib))

    # é‚»å±…ä¿¡æ¯æ­å»º  S å’Œ T
    def neighbor_information(self, uiMat, n_users):
        uimat = uiMat[: n_users, n_users:]
        # å°† uimat è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¼ é‡ï¼Œé¦–å…ˆä½¿ç”¨ .tocoo() æ–¹æ³•å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºåæ ‡æ ¼å¼ (COO)ï¼Œç„¶åæå–æ•°æ®éƒ¨åˆ†ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º PyTorch æµ®ç‚¹æ•°å¼ é‡ã€‚
        values = t.FloatTensor(uimat.tocoo().data)  # å°†uimatè½¬æ¢ä¸ºæµ®ç‚¹æ•°å¼ é‡
        # è·å–ç¨€ç–çŸ©é˜µä¸­éé›¶å…ƒç´ çš„è¡Œç´¢å¼•å’Œåˆ—ç´¢å¼•ï¼Œç„¶åå°†å®ƒä»¬å‚ç›´å †å åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ª 2xN çš„ NumPy æ•°ç»„ï¼Œå…¶ä¸­ N æ˜¯éé›¶å…ƒç´ çš„æ•°é‡ã€‚
        indices = np.vstack((uimat.tocoo().row, uimat.tocoo().col))
        i = t.LongTensor(indices)
        v = t.FloatTensor(values)
        shape = uimat.tocoo().shape
        # è¿™ä¸ªç¨€ç–å¼ é‡è¡¨ç¤ºäº†åŸå§‹ç¨€ç–çŸ©é˜µçš„éƒ¨åˆ†ï¼Œå…¶éé›¶å…ƒç´ å’Œå½¢çŠ¶ä¸åŸå§‹çŸ©é˜µç›¸åŒã€‚
        uimat1 = t.sparse.FloatTensor(i, v, t.Size(shape))
        uiadj = uimat1  # è¿™ä¸ªå¼ é‡è¡¨ç¤ºç”¨æˆ·åˆ°ç‰©å“çš„è¿æ¥å…³ç³»
        iuadj = uimat1.transpose(0, 1)  # å®ƒè¡¨ç¤ºç‰©å“åˆ°ç”¨æˆ·çš„è¿æ¥å…³ç³»ã€‚é€šè¿‡å°†ç¨€ç–å¼ é‡çš„ç»´åº¦è¿›è¡Œè½¬ç½®ï¼Œå¯ä»¥å®ç°è¿™ä¸€æ“ä½œã€‚
        return uiadj, iuadj

    # æ‰§è¡Œå…ƒè·¯å¾„çŸ¥è¯†æŠ½å–å’Œä¸ªæ€§åŒ–è½¬æ¢  self.userEmbedding, self.ui_userEmbedding, self.itemEmbedding, self.ui_itemEmbedding
    def metafortansform(self, uiMat, n_users, auxiembedu, targetembedu, auxiembedi, targetembedi):
        uiadj, iuadj = self.neighbor_information(uiMat, n_users)

        # Neighbor information of the target node  è¿™ä¸¤è¡Œä»£ç è®¡ç®—äº†ç”¨æˆ·å’Œç‰©å“çš„é‚»å±…ä¿¡æ¯ã€‚
        uneighbor = t.matmul(uiadj.to(self.device), targetembedi)  # åŒ…å«äº†ç”¨æˆ·çš„ç‰©å“é‚»å±…çš„ä¿¡æ¯
        ineighbor = t.matmul(iuadj.to(self.device), targetembedu)  # åŒ…å«äº†ç‰©å“çš„ç”¨æˆ·é‚»å±…çš„ä¿¡æ¯

        # Meta-knowlege extraction  è¿™ä¸¤è¡Œä»£ç ä½¿ç”¨å…ƒè·¯å¾„æ³¨æ„åŠ›æ¨¡å‹æ¥æŠ½å–ç”¨æˆ·å’Œç‰©å“çš„å…ƒè·¯å¾„çŸ¥è¯†
        tembedu = ( self.meta_netu(t.cat((auxiembedu, targetembedu, uneighbor), dim=1).detach()))  # é™„åŠ çš„é¢†åŸŸä¿¡æ¯æ˜¾å¼åœ°å¢å¼ºäº†ç›´æ¥å›¾è¿æ¥çš„å»ºæ¨¡
        tembedi = (self.meta_neti(t.cat((auxiembedi, targetembedi, ineighbor), dim=1).detach()))

        """ Personalized transformation parameter matrix """
        # Low rank matrix decomposition
        metau1 = self.mlp(tembedu).reshape(-1, self.hide_dim, self.k)  # d*k  #  è¿™äº›è¡Œæ‰§è¡Œäº†ä½ç§©çŸ©é˜µåˆ†è§£ï¼Œå°†å…ƒè·¯å¾„çŸ¥è¯†è½¬æ¢ä¸ºæƒé‡çŸ©é˜µã€‚è¿™äº›æƒé‡çŸ©é˜µå°†ç”¨äºä¸ªæ€§åŒ–è½¬æ¢ã€‚
        metau2 = self.mlp1(tembedu).reshape(-1, self.k, self.hide_dim)  # k*d
        metai1 = self.mlp2(tembedi).reshape(-1, self.hide_dim, self.k)  # d*k
        metai2 = self.mlp3(tembedi).reshape(-1, self.k, self.hide_dim)  # k*d

        meta_biasu = (t.mean(metau1, dim=0))  # è¿™äº›è¡Œè®¡ç®—äº†å…ƒè·¯å¾„æƒé‡çš„åç½®é¡¹ï¼Œç”¨äºåŠ æƒå¹³å‡å…ƒè·¯å¾„çŸ¥è¯†ã€‚
        meta_biasu1 = (t.mean(metau2, dim=0))
        meta_biasi = (t.mean(metai1, dim=0))
        meta_biasi1 = (t.mean(metai2, dim=0))

        low_weightu1 = F.softmax(metau1 + meta_biasu, dim=1)  # è¿™äº›è¡Œä½¿ç”¨ softmax å‡½æ•°å¯¹å…ƒè·¯å¾„æƒé‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥ç¡®ä¿å®ƒä»¬çš„å’Œä¸º1
        low_weightu2 = F.softmax(metau2 + meta_biasu1, dim=1)
        low_weighti1 = F.softmax(metai1 + meta_biasi, dim=1)
        low_weighti2 = F.softmax(metai2 + meta_biasi1, dim=1)

        # The learned matrix as the weights of the transformed network
        # è¿™äº›è¡Œæ ¹æ®å…ƒè·¯å¾„æƒé‡å°†è¾“å…¥çš„ç”¨æˆ·å’Œç‰©å“åµŒå…¥è¿›è¡Œä¸ªæ€§åŒ–è½¬æ¢ã€‚
        # tembedus åŒ…å«äº†è½¬æ¢åçš„ç”¨æˆ·åµŒå…¥ï¼Œè€Œ tembedis åŒ…å«äº†è½¬æ¢åçš„ç‰©å“åµŒå…¥ã€‚è¿™äº›ä¸ªæ€§åŒ–è½¬æ¢åçš„åµŒå…¥å°†ç”¨äºæ›´æ–°ç”¨æˆ·å’Œç‰©å“çš„åµŒå…¥ã€‚
        tembedus = (t.sum(t.multiply((auxiembedu).unsqueeze(-1), low_weightu1), dim=1))  # Equal to a two-layer linear network; Ciao and Yelp data sets are plus gelu activation function
        tembedus = t.sum(t.multiply((tembedus).unsqueeze(-1), low_weightu2), dim=1)
        tembedis = (t.sum(t.multiply((auxiembedi).unsqueeze(-1), low_weighti1), dim=1))
        tembedis = t.sum(t.multiply((tembedis).unsqueeze(-1), low_weighti2), dim=1)
        transfuEmbed = tembedus
        transfiEmbed = tembedis
        return transfuEmbed, transfiEmbed

    # å¼‚æ„ä¿¡æ¯ä¼ æ’­å’Œèšåˆ
    def information_dissemination_aggregation(self, n_users, n_items, ua_embeddings, ia_embeddings, uuMat, iiMat, uiMat, norm = 1):
        # HGCL
        user_index = np.arange(0, n_users)  # åˆ›å»ºä¸€ä¸ªç‰©å“æºåŸŸç´¢å¼•æ•°ç»„
        item_index = np.arange(0, n_items)  # åˆ›å»ºä¸€ä¸ªç”¨æˆ·æºåŸŸç´¢å¼•æ•°ç»„
        # åˆ›å»ºä¸€ä¸ªåŒ…å«ç”¨æˆ·ç´¢å¼•å’Œç‰©å“ç´¢å¼•çš„æ•°ç»„ã€‚ç‰©å“ç´¢å¼•ä¼šè¢«åç§» self.userNumï¼Œä»¥ä¸ç”¨æˆ·ç´¢å¼•åŒºåˆ†å¼€
        ui_index = np.array(user_index.tolist() + [i + n_users for i in item_index])

        # Initialize Embeddings åˆå§‹åŒ–åµŒå…¥
        uu_embed0 = self.self_gatingu(ua_embeddings)  # é€šè¿‡è‡ªä¸»é—¨æ§æœºåˆ¶ï¼Œå¾—åˆ°ç”¨æˆ·åˆ°ç”¨æˆ·çš„åµŒå…¥ æºåŸŸ
        ii_embed0 = self.self_gatingi(ia_embeddings)
        self.ui_embed0 = t.cat([ua_embeddings, ia_embeddings], 0)  # å°†ç”¨æˆ·åµŒå…¥å’Œç‰©å“åµŒå…¥æŒ‰è¡Œè¿æ¥ï¼Œè¡¨ç¤ºç”¨æˆ·å’Œç‰©å“çš„è”åˆåµŒå…¥
        self.all_user_embeddings = [uu_embed0]  # åˆå§‹åŒ–äº†ç”¨æˆ·å’Œç‰©å“çš„åµŒå…¥åˆ—è¡¨ï¼Œå°†åˆå§‹åµŒå…¥æ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚
        self.all_item_embeddings = [ii_embed0]
        self.all_ui_embeddings = [self.ui_embed0]

        # Encoder  ç”¨äºå¤„ç†æ¯ä¸ª GCN å±‚, å¼‚æ„æ¶ˆæ¯ä¼ æ’­
        for i in range(len(self.encoder)):
            layer = self.encoder[i]  # ä»æ¨¡å‹çš„ GCN å±‚åˆ—è¡¨ä¸­è·å–å½“å‰å±‚ i å¯¹åº”çš„ GCN å±‚
            if i == 0:
                userEmbeddings0 = layer(uu_embed0, uuMat, user_index)
                itemEmbeddings0 = layer(ii_embed0, iiMat, item_index)
                uiEmbeddings0 = layer(self.ui_embed0, uiMat, ui_index)
            else:
                # å¦‚æœä¸æ˜¯ç¬¬ä¸€å±‚ï¼ˆå³ else åˆ†æ”¯ï¼‰ï¼Œåˆ™ä½¿ç”¨å‰ä¸€å±‚çš„åµŒå…¥ä½œä¸ºè¾“å…¥ï¼Œå†æ¬¡åº”ç”¨å½“å‰å±‚çš„ GCNã€‚
                userEmbeddings0 = layer(userEmbeddings, uuMat, user_index)
                itemEmbeddings0 = layer(itemEmbeddings, iiMat, item_index)
                uiEmbeddings0 = layer(uiEmbeddings, uiMat, ui_index)

            # å¼‚æ„ä¿¡æ¯èšåˆ
            # Aggregation of message features across the two related views in the middle layer then fed into the next layer
            # å°†è”åˆåµŒå…¥ uiEmbeddings0 åˆ†å‰²æˆç”¨æˆ·åµŒå…¥å’Œç‰©å“åµŒå…¥ï¼Œåˆ†å‰²çš„æ–¹å¼æ˜¯æ ¹æ®ç”¨æˆ·æ•°å’Œç‰©å“æ•°è¿›è¡Œåˆ†å‰²
            self.ui_userEmbedding0, self.ui_itemEmbedding0 = t.split(uiEmbeddings0, [n_users, n_items])
            userEd = (userEmbeddings0 + self.ui_userEmbedding0) / 2.0  # å°†å½“å‰å±‚å¤„ç†åçš„ç”¨æˆ·åµŒå…¥ä¸è”åˆåµŒå…¥çš„ç”¨æˆ·éƒ¨åˆ†æ±‚å¹³å‡
            itemEd = (itemEmbeddings0 + self.ui_itemEmbedding0) / 2.0

            userEmbeddings = userEd
            itemEmbeddings = itemEd
            uiEmbeddings = t.cat([userEd, itemEd], 0)  # æ›´æ–°è”åˆåµŒå…¥ uiEmbeddingsï¼Œå°†å…¶è®¾ç½®ä¸ºåˆå¹¶åçš„ç”¨æˆ·åµŒå…¥å’Œç‰©å“åµŒå…¥ã€‚

            if norm == 1:  # é€‰æ‹©æ˜¯å¦å¯¹æ–°åµŒå…¥è¿›è¡Œ L2 å½’ä¸€åŒ–æ“ä½œ
                norm_embeddings = F.normalize(userEmbeddings0, p=2, dim=1)
                self.all_user_embeddings += [norm_embeddings]
                norm_embeddings = F.normalize(itemEmbeddings0, p=2, dim=1)
                self.all_item_embeddings += [norm_embeddings]
                norm_embeddings = F.normalize(uiEmbeddings0, p=2, dim=1)
                self.all_ui_embeddings += [norm_embeddings]
            else:
                self.all_user_embeddings += [userEmbeddings]
                self.all_item_embeddings += [itemEmbeddings0]
                self.all_ui_embeddings += [uiEmbeddings0]

        self.userEmbedding = t.stack(self.all_user_embeddings, dim=1)  # æ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç”¨æˆ·çš„åµŒå…¥
        self.userEmbedding = t.mean(self.userEmbedding, dim=1)  # å°†æ‰€æœ‰ç”¨æˆ·çš„åµŒå…¥åˆå¹¶æˆä¸€ä¸ªå‡å€¼å‘é‡ï¼Œä»£è¡¨æ•´ä½“çš„ç”¨æˆ·åµŒå…¥ã€‚
        self.itemEmbedding = t.stack(self.all_item_embeddings, dim=1)  # æ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç‰©å“çš„åµŒå…¥
        self.itemEmbedding = t.mean(self.itemEmbedding, dim=1)
        self.uiEmbedding = t.stack(self.all_ui_embeddings, dim=1)
        self.uiEmbedding = t.mean(self.uiEmbedding, dim=1)

        # åˆ†å‰²  æå–ç”¨æˆ·å’Œé¡¹ç›®çš„å…ƒçŸ¥è¯†
        self.ui_userEmbedding, self.ui_itemEmbedding = t.split(self.uiEmbedding, [n_users, n_items])

        # Personalized Transformation of Auxiliary Domain Features
        # è¯¥å‡½æ•°æ‰§è¡Œå…ƒè·¯å¾„æ³¨æ„åŠ›çš„å˜æ¢æ“ä½œï¼Œç„¶åè¿”å›ä¸¤ä¸ªç»“æœï¼šmetatsuembed è¡¨ç¤ºå˜æ¢åçš„ç”¨æˆ·åµŒå…¥ï¼Œmetatsiembed è¡¨ç¤ºå˜æ¢åçš„ç‰©å“åµŒå…¥
        # å°†æ‰§è¡Œå…ƒè·¯å¾„æ³¨æ„åŠ›å˜æ¢åçš„ç”¨æˆ·å’Œç‰©å“åµŒå…¥åŠ å…¥åˆ°åŸå§‹åµŒå…¥ä¸­ï¼Œä»¥è·å¾—æ›´æ–°åçš„ç”¨æˆ·å’Œç‰©å“åµŒå…¥ã€‚
        # ï¼ï¼ï¼æœ€ç»ˆåµŒå…¥ï¼ï¼ï¼
        metatsuembed, metatsiembed = self.metafortansform(uiMat, n_users, self.userEmbedding, self.ui_userEmbedding, self.itemEmbedding, self.ui_itemEmbedding)

        # ï¼ï¼ï¼æœ€ç»ˆåµŒå…¥ï¼ï¼ï¼
        self.userEmbedding = self.userEmbedding + metatsuembed
        self.itemEmbedding = self.itemEmbedding + metatsiembed


        return self.userEmbedding, self.itemEmbedding, self.ui_userEmbedding, self.ui_itemEmbedding

    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):
        """Convert a scipy sparse mattrix to a torch sparse tensor."""
        if type(sparse_mx) != sp.coo_matrix:
            sparse_mx = sparse_mx.tocoo().astype(np.float32)
        indices = t.from_numpy(
            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))
        values = t.from_numpy(sparse_mx.data).float()
        shape = t.Size(sparse_mx.shape)
        return t.sparse.FloatTensor(indices, values, shape)

    # å…ƒè·¯å¾„æ­£åˆ™åŒ–çš„è®¡ç®—è¿‡ç¨‹
    # self.ui_userEmbedding[uid.cpu().numpy()],   (self.userEmbedding),   self.uuMat[uid.cpu().numpy()]
    def metaregular(self, em0, em, adj):
        # ç”¨äºå¯¹åµŒå…¥å‘é‡è¿›è¡Œè¡Œå’Œåˆ—çš„éšæœºé‡æ’ã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†ç”Ÿæˆè´Ÿæ ·æœ¬ã€‚
        def row_column_shuffle(embedding):
            corrupted_embedding = embedding[:, t.randperm(embedding.shape[1])]
            corrupted_embedding = corrupted_embedding[t.randperm(embedding.shape[0])]
            return corrupted_embedding

        # ç”¨äºè®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œè¿™é‡Œä½¿ç”¨ç‚¹ç§¯ã€‚
        def score(x1, x2):
            x1 = F.normalize(x1, p=2, dim=-1)
            x2 = F.normalize(x2, p=2, dim=-1)
            return t.sum(t.multiply(x1, x2), 1)

        user_embeddings = em

        # è®¡ç®—é‚»æ¥çŸ©é˜µ adj ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„åº¦ï¼ˆåº¦æ˜¯æŒ‡ä¸è¯¥èŠ‚ç‚¹ç›¸è¿çš„è¾¹çš„æ•°é‡ï¼‰ï¼Œå¹¶å°†ç»“æœè½¬æ¢ä¸ºPyTorchå¼ é‡
        Adj_Norm = t.from_numpy(np.sum(adj, axis=1)).float().to(self.device)
        adj = self.sparse_mx_to_torch_sparse_tensor(adj)  # å°†é‚»æ¥çŸ©é˜µ adj è½¬æ¢ä¸ºç¨€ç–å¼ é‡ï¼ˆPyTorchçš„ç¨€ç–çŸ©é˜µè¡¨ç¤ºï¼‰ã€‚
        edge_embeddings = t.spmm(adj.to(self.device), user_embeddings) / Adj_Norm  # è®¡ç®—èŠ‚ç‚¹åµŒå…¥å’Œè¾¹åµŒå…¥çš„ä¹˜ç§¯ï¼Œç„¶åé™¤ä»¥èŠ‚ç‚¹åº¦ä»¥è·å¾—å½’ä¸€åŒ–çš„è¾¹åµŒå…¥ã€‚
        user_embeddings = em0  # è¿™æ˜¯ä¸ºäº†ç”Ÿæˆæ­£æ ·æœ¬ã€‚
        graph = t.mean(edge_embeddings, 0)  # è®¡ç®—æ‰€æœ‰è¾¹åµŒå…¥çš„å¹³å‡å€¼ï¼Œå¾—åˆ°ä¸€ä¸ªå…¨å±€çš„å›¾åµŒå…¥ã€‚
        pos = score(user_embeddings, graph)  # è®¡ç®—æ­£æ ·æœ¬çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œè¿™é‡Œæ˜¯ ç”¨æˆ·åµŒå…¥ ä¸ å…¨å±€å›¾åµŒå…¥ çš„ç›¸ä¼¼åº¦ã€‚
        neg1 = score(row_column_shuffle(user_embeddings), graph)  # ç”Ÿæˆè´Ÿæ ·æœ¬ï¼Œé¦–å…ˆå¯¹ç”¨æˆ·åµŒå…¥è¿›è¡Œéšæœºé‡æ’ï¼Œç„¶åè®¡ç®—ä¸å…¨å±€å›¾åµŒå…¥çš„ç›¸ä¼¼åº¦ã€‚
        global_loss = t.mean(-t.log(t.sigmoid(pos - neg1)))  # è®¡ç®—å…ƒè·¯å¾„æ­£åˆ™åŒ–çš„å…¨å±€æŸå¤±ï¼Œè¿™é‡Œä½¿ç”¨äºŒå…ƒäº¤å‰ç†µ æŸå¤±å‡½æ•°
        return global_loss

    # å®šä¹‰è®¡ç®— BPRï¼ˆBayesian Personalized Rankingï¼‰æŸå¤±çš„å‡½æ•°ï¼Œç”¨äºè¡¡é‡æ¨¡å‹çš„è®­ç»ƒæ•ˆæœ
    def create_bpr_loss(self, users, pos_items, neg_items):
        pos_scores = t.sum(t.mul(users, pos_items), axis=1)  # é€šè¿‡ç”¨æˆ·åµŒå…¥å‘é‡å’Œæ­£æ ·æœ¬ç‰©å“åµŒå…¥å‘é‡çš„ç‚¹ç§¯ï¼Œè®¡ç®—å‡ºæ­£æ ·æœ¬çš„å¾—åˆ†ã€‚
        neg_scores = t.sum(t.mul(users, neg_items), axis=1)  # é€šè¿‡ç”¨æˆ·åµŒå…¥å‘é‡å’Œè´Ÿæ ·æœ¬ç‰©å“åµŒå…¥å‘é‡çš„ç‚¹ç§¯ï¼Œè®¡ç®—å‡ºè´Ÿæ ·æœ¬çš„å¾—åˆ†ã€‚

        # è®¡ç®—åŸºäº BPR çš„ MFï¼ˆMatrix Factorizationï¼‰æŸå¤±ã€‚è¿™é‡Œä½¿ç”¨äº† softplus å‡½æ•°ï¼Œå®ƒå¯ä»¥å°†ä»»æ„è¾“å…¥æ˜ å°„åˆ°éè´Ÿå€¼ï¼Œä»¥ç¡®ä¿æŸå¤±å§‹ç»ˆä¸ºæ­£ã€‚
        mf_loss = t.mean(t.nn.functional.softplus(neg_scores - pos_scores))
        # mf_loss = t.mean(-t.log(t.sigmoid(pos_scores - neg_scores)))  # äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°
        return mf_loss

# ä¸æŒ‰ç…§æ‰¹æ¬¡[uid.cpu().numpy()]è®¡ç®—å…ƒè·¯å¾„æ­£åˆ™åŒ–æŸå¤±
    def calculate_metaregloss(self, ui_userEmbedding, userEmbedding, ui_itemEmbedding, itemEmbedding, uuMat, iiMat):
        # Regularization: the constraint of transformed reasonableness
        self.reg_lossu = self.metaregular(ui_userEmbedding, userEmbedding, uuMat)
        self.reg_lossi = self.metaregular(ui_itemEmbedding, itemEmbedding, iiMat)
        metaregloss = (self.reg_lossu + self.reg_lossi) / 2.0 # å…ƒè·¯å¾„æ­£åˆ™åŒ–æŸå¤±

        return metaregloss

    # è¿™æ®µä»£ç çš„ç›®çš„æ˜¯è®¡ç®— SSLï¼ˆSemi-Supervised Learningï¼‰æŸå¤±ï¼Œç”¨äºè®­ç»ƒä¸­çš„åŠç›‘ç£å­¦ä¹ ã€‚
    # SSLæŸå¤±å¯ä»¥å¸®åŠ©åœ¨è·¨åŸŸæ¨èä¸­å­¦ä¹ ä¸åŒåŸŸä¹‹é—´çš„å…±äº«ä¿¡æ¯å’Œç‰¹å®šä¿¡æ¯
    def calc_ssl_loss_strategy(self, layer_embed_1, layer_embed_2, n_users_1, n_users_2, n_items_1, n_items_2):
        # # åˆ†å‰²   S
        # users_1, items_1 = t.split(layer_embed_1, [n_users_1, n_items_1])
        # users_up_1, items_up_1, _, _ = self.information_dissemination_aggregation(n_users_1, n_items_1, users_1, items_1, self.uuMat_1, self.iiMat_1, self.uiMat_1)
        # # ç»„åˆ   S
        # final_layer_embed_1 = t.cat([users_up_1, items_up_1])
        #
        # # åˆ†å‰²   T
        # users_2, items_2 = t.split(layer_embed_2, [n_users_2, n_items_2])
        # users_up_2, items_up_2, _, _ = self.information_dissemination_aggregation(n_users_2, n_items_2, users_2, items_2, self.uuMat_2, self.iiMat_2, self.uiMat_2)
        # # ç»„åˆ   T
        # final_layer_embed_2 = t.cat([users_up_2, items_up_2])

        # æå–ä¸å˜å’Œç‰¹å®šä¿¡æ¯çš„åµŒå…¥å‘é‡
        invariant_embed_1, specific_embed_1 = layer_embed_1[0], layer_embed_1[1]
        # ä»ä¸­æå–ç”¨æˆ·çš„éƒ¨åˆ†
        invariant_u_embed_1, specific_u_embed_1 = invariant_embed_1[:n_users_1], specific_embed_1[:n_users_1]

        invariant_embed_2, specific_embed_2 = layer_embed_2[0], layer_embed_2[1]
        invariant_u_embed_2, specific_u_embed_2 = invariant_embed_2[:n_users_2], specific_embed_2[:n_users_2]

        # å‘é‡æ ‡å‡†åŒ–ï¼Œ å¯¹æå–çš„ä¸å˜ä¿¡æ¯  ç”¨æˆ·åµŒå…¥å‘é‡  è¿›è¡ŒL2 å½’ä¸€åŒ–ï¼Œç¡®ä¿å‘é‡çš„èŒƒæ•°ä¸º1
        normalize_invariant_user_1 = t.nn.functional.normalize(invariant_u_embed_1, p=2, dim=1)
        normalize_invariant_user_2 = t.nn.functional.normalize(invariant_u_embed_2, p=2, dim=1)

        normalize_specific_user_1 = t.nn.functional.normalize(specific_u_embed_1, p=2, dim=1)
        normalize_specific_user_2 = t.nn.functional.normalize(specific_u_embed_2, p=2, dim=1)

        # è®¡ç®—ä¸¤ä¸ªä¸åŒåŸŸçš„ä¸å˜ä¿¡æ¯ç”¨æˆ·åµŒå…¥å‘é‡çš„ç‚¹ç§¯ï¼Œä½œä¸ºæ­£æ ·æœ¬çš„å¾—åˆ†ã€‚æ­£æ ·æœ¬ä»£è¡¨çš„æ˜¯ä¸åŒåŸŸä¹‹é—´å…±äº«ä¿¡æ¯çš„ç›¸ä¼¼æ€§ã€‚
        pos_score_user = t.sum(t.mul(normalize_invariant_user_1, normalize_invariant_user_2), dim=1)

        # è®¡ç®—ä¸åŒç»„åˆçš„ä¸å˜ä¿¡æ¯å’Œç‰¹å®šä¿¡æ¯ç”¨æˆ·åµŒå…¥å‘é‡çš„ç‚¹ç§¯ï¼Œä½œä¸ºè´Ÿæ ·æœ¬çš„å¾—åˆ†ã€‚ç‰¹å®šä¿¡æ¯å’Œä¸åŒåŸŸä¹‹é—´çš„å·®å¼‚æ€§
        # æ ‡å‡†çš„ä½™å¼¦ç›¸ä¼¼åº¦ä»£ç ï¼šcosine_similarity = torch.sum(normalize_invariant_user_1 * normalize_specific_user_1, dim=1)
        neg_score_1 = t.sum(t.mul(normalize_invariant_user_1, normalize_specific_user_1), dim=1)
        neg_score_2 = t.sum(t.mul(normalize_invariant_user_2, normalize_specific_user_2), dim=1)
        neg_score_3 = t.sum(t.mul(normalize_specific_user_1, normalize_specific_user_2), dim=1)

        # ä¸ºä»€ä¹ˆä¸è®¡ç®—
        # neg_score_5 = torch.sum(torch.mul(normalize_invariant_user_2, normalize_specific_user_1), dim=1)

        # ä½¿ç”¨çŸ©é˜µä¹˜æ³•è®¡ç®—ä¸¤ä¸ªä¸å˜ä¿¡æ¯ç”¨æˆ·åµŒå…¥å‘é‡ä¹‹é—´çš„ç‚¹ç§¯ï¼Œä½œä¸ºé¢å¤–çš„è´Ÿæ ·æœ¬å¾—åˆ†ã€‚
        # å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°åŒºåˆ†ä¸åŒåŸŸä¹‹é—´çš„ç”¨æˆ·å…³ç³»
        neg_score_4 = t.matmul(normalize_invariant_user_2, normalize_specific_user_1.T)

        # å¯¹å¾—åˆ†è¿›è¡ŒæŒ‡æ•°åŒ–ï¼Œä»¥ä¾¿è¿›è¡Œåç»­çš„æŸå¤±è®¡ç®—ã€‚
        # self.ssl_temp æ§åˆ¶äº†æŸå¤±å‡½æ•°ä¸­æŒ‡æ•°åŒ–å¾—åˆ†çš„æ¸©åº¦ï¼Œå½±å“äº†å¾—åˆ†ä¹‹é—´çš„ç›¸å¯¹å¤§å°ã€‚
        # æ¸©åº¦å‚æ•°æ˜¯ç”¨æ¥è°ƒæ•´ softmax æ“ä½œçš„è¾“å‡ºï¼Œä½¿å…¶æ›´å¹³æ»‘æˆ–æ›´å°–é”ã€‚
        # åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œself.ssl_temp çš„å€¼ä¸º 0.1ï¼Œæ„å‘³ç€åœ¨è®¡ç®— SSL æŸå¤±æ—¶å°†ä½¿ç”¨è¾ƒä½çš„æ¸©åº¦ã€‚
        pos_score = t.exp(pos_score_user / self.ssl_temp)
        neg_score_1 = t.exp(neg_score_1 / self.ssl_temp)
        neg_score_2 = t.exp(neg_score_2 / self.ssl_temp)
        neg_score_3 = t.exp(neg_score_3 / self.ssl_temp)
        neg_score_4 = t.sum(t.exp(neg_score_4 / self.ssl_temp), dim=1)

        # æœ€å°åŒ–æ­£æ ·æœ¬å¾—åˆ†ä¸æ‰€æœ‰è´Ÿæ ·æœ¬å¾—åˆ†çš„æ¯”å€¼çš„è´Ÿå¯¹æ•°ï¼Œä»¥ä¾¿æé«˜æ­£æ ·æœ¬å¾—åˆ†å¹¶é™ä½è´Ÿæ ·æœ¬å¾—åˆ†ï¼Œä»è€Œæ”¹å–„æ¨¡å‹çš„åŒºåˆ†æ€§èƒ½
        ssl_loss_user = -t.sum(t.log(pos_score / (neg_score_1 + neg_score_2 + neg_score_3 + pos_score +
                                                          neg_score_4)))
        # SSL æŸå¤±å’Œæ¸©åº¦å‚æ•°ä¸€èµ·ï¼Œå¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å­¦ä¹  è·¨åŸŸæ¨èä¸­ç”¨æˆ·ä¹‹é—´çš„å…±äº«ä¿¡æ¯å’Œç‰¹å®šä¿¡æ¯
        ssl_loss = ssl_loss_user
        return ssl_loss

    # Contrastive Learning
    def ssl_loss(self, data1, data2, index):
        # æ˜¯ä¸€ä¸ªåŒ…å«ç´¢å¼•çš„å¼ é‡ï¼Œå®ƒç”¨äºæŒ‡ç¤ºåº”è¯¥åœ¨è¾“å…¥æ•°æ®ä¸­é€‰æ‹©å“ªäº›æ ·æœ¬è¿›è¡Œè®¡ç®—
        index = t.from_numpy(index)
        index = t.unique(index)  # é¦–å…ˆç¡®ä¿ç´¢å¼• index ä¸­çš„å€¼ä¸é‡å¤ï¼Œè¿™å¯ä»¥é¿å…é‡å¤è®¡ç®—ç›¸åŒçš„æ ·æœ¬å¯¹
        embeddings1 = data1[index]
        embeddings2 = data2[index]
        norm_embeddings1 = F.normalize(embeddings1, p=2, dim=1)
        norm_embeddings2 = F.normalize(embeddings2, p=2, dim=1)
        pos_score = t.sum(t.mul(norm_embeddings1, norm_embeddings2), dim=1)  # æ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œè¿™é‡Œé‡‡ç”¨äº†å‘é‡ç‚¹ç§¯æ¥åº¦é‡å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
        all_score = t.mm(norm_embeddings1, norm_embeddings2.T)  # æ‰€æœ‰æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦åˆ†æ•°çŸ©é˜µï¼Œå…¶ä¸­çš„æ¯ä¸ªå…ƒç´ è¡¨ç¤ºä¸€ä¸ªæ ·æœ¬å¯¹çš„ç›¸ä¼¼æ€§ã€‚
        pos_score = t.exp(pos_score / self.ssl_temp_h)  # åº”ç”¨äº† softmax æ“ä½œï¼Œè¿™æœ‰åŠ©äºå°†åˆ†æ•°è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        all_score = t.sum(t.exp(all_score / self.ssl_temp_h), dim=1)
        # ç›®æ ‡æ˜¯ä½¿æ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦åˆ†æ•°æ›´å¤§ï¼Œè€Œå°†å…¶ä»–æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦åˆ†æ•°å°½å¯èƒ½åœ°é™ä½
        ssl_loss = (-t.sum(t.log(pos_score / ((all_score)))) / (len(index)))
        return ssl_loss

    def predictModel(user, pos_i, neg_j, isTest=False):
        if isTest:
            pred_pos = t.sum(user * pos_i, dim=1)
            return pred_pos
        else:
            pred_pos = t.sum(user * pos_i, dim=1)
            pred_neg = t.sum(user * neg_j, dim=1)
            return pred_pos, pred_neg

    # å‰å‘ä¼ æ’­å‡½æ•°ï¼Œåˆ†åˆ«æ˜¯ï¼šç”¨æˆ·-é¡¹ç›®äº¤äº’æ•°æ®ä¸­çš„ç”¨æˆ·ç´¢å¼•ï¼ˆç”¨æˆ·IDï¼‰ã€ç”¨æˆ·-é¡¹ç›®äº¤äº’æ•°æ®ä¸­çš„é¡¹ç›®ç´¢å¼•ï¼ˆæ­£æ ·æœ¬ï¼‰ã€éšæœºé¡¹ç›®ç´¢å¼•ï¼ˆè´Ÿæ ·æœ¬ï¼‰
    def forward(self, users_1, pos_items_1, neg_items_1, users_2, pos_items_2, neg_items_2, userindex_S, itemindex_S, userindex_T, itemindex_T, train):
        # ç”ŸæˆåµŒå…¥å‘é‡
        self.ua_embeddings_1, self.ia_embeddings_1, self.layer_embeddings_1 = self.computer(self.Graph_1, self.embedding_dict['user_embedding_S'],
                                                                         self.embedding_dict['item_embedding_S'], self.n_users_1, self.n_items_1)
        self.ua_embeddings_2, self.ia_embeddings_2, self.layer_embeddings_2 = self.computer(self.Graph_2, self.embedding_dict['user_embedding_T'],
                                                                         self.embedding_dict['item_embedding_T'], self.n_users_2, self.n_items_2)

        # èåˆäº†å¼‚æ„ä¿¡æ¯ä¹‹åå¾—åˆ°çš„ç”¨æˆ·ã€ç‰©å“ã€layeråµŒå…¥å‘é‡ uu ii ui_u ui_i
        self.userEmbedding_1, self.itemEmbedding_1, self.ui_userEmbedding_1, self.ui_itemEmbedding_1 = self.information_dissemination_aggregation(self.n_users_1,
                                                                                                self.n_items_1,
                                                                                                self.ua_embeddings_1,
                                                                                                self.ia_embeddings_1,
                                                                                                self.uuMat_1,
                                                                                                self.iiMat_1,
                                                                                                self.uiMat_1)
        self.userEmbedding_2, self.itemEmbedding_2, self.ui_userEmbedding_2, self.ui_itemEmbedding_2  = self.information_dissemination_aggregation(self.n_users_2,
                                                                                                self.n_items_2,
                                                                                                self.ua_embeddings_2,
                                                                                                self.ia_embeddings_2,
                                                                                                self.uuMat_2,
                                                                                                self.iiMat_2,
                                                                                                self.uiMat_2)

        self.fu1 = self.wu1 * self.ui_userEmbedding_1 + self.wu2 * self.userEmbedding_1
        self.fi1 = self.wi1 * self.ui_itemEmbedding_1 + self.wi2 * self.itemEmbedding_1
        self.fu2 = self.wu1 * self.ui_userEmbedding_2 + self.wu2 * self.userEmbedding_2
        self.fi2 = self.wi1 * self.ui_itemEmbedding_2 + self.wi2 * self.itemEmbedding_2

        # æ ¹æ®ç´¢å¼•ä»ä¸åŒçš„åµŒå…¥çŸ©é˜µä¸­æå–å‡ºå¯¹åº”çš„ç”¨æˆ·å’Œæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„åµŒå…¥å‘é‡ï¼Œä»¥åŠé¢„è®­ç»ƒçš„......
        self.u_g_embeddings_1 = t.index_select(self.fu1, 0, t.LongTensor(users_1).to(self.device))
        self.pos_i_g_embeddings_1 = t.index_select(self.fi1, 0, t.LongTensor(pos_items_1).to(self.device))
        self.neg_i_g_embeddings_1 = t.index_select(self.fi1, 0, t.LongTensor(neg_items_1).to(self.device))
        # self.u_g_embeddings_pre_1 = t.index_select(self.embedding_dict['user_embedding_S'], 0, t.LongTensor(users_1).to(self.device))
        # self.pos_i_g_embeddings_pre_1 = t.index_select(self.embedding_dict['item_embedding_S'], 0, t.LongTensor(pos_items_1).to(self.device))
        # self.neg_i_g_embeddings_pre_1 = t.index_select(self.embedding_dict['item_embedding_S'], 0, t.LongTensor(neg_items_1).to(self.device))

        self.u_g_embeddings_2 = t.index_select(self.fu2, 0, t.LongTensor(users_2).to(self.device))
        self.pos_i_g_embeddings_2 = t.index_select(self.fi2, 0, t.LongTensor(pos_items_2).to(self.device))
        self.neg_i_g_embeddings_2 = t.index_select(self.fi2, 0, t.LongTensor(neg_items_2).to(self.device))
        # self.u_g_embeddings_pre_2 = t.index_select(self.embedding_dict['user_embedding_T'], 0, t.LongTensor(users_2).to(self.device))
        # self.pos_i_g_embeddings_pre_2 = t.index_select(self.embedding_dict['item_embedding_T'], 0, t.LongTensor(pos_items_2).to(self.device))
        # self.neg_i_g_embeddings_pre_2 = t.index_select(self.embedding_dict['item_embedding_T'], 0, t.LongTensor(neg_items_2).to(self.device))

        metaregloss_1 = 0  # å…ƒè·¯å¾„æ­£åˆ™åŒ–æŸå¤±
        metaregloss_2 = 0
        variationloss = 0 # dccdr LOSS
        ssl_loss_T = 0
        ssl_loss_S = 0
        regLoss_S = 0
        regLoss_T = 0

        if train == True:  # åªæœ‰åœ¨è®­ç»ƒæ¨¡å¼ä¸‹æ‰ä¼šè®¡ç®—å…ƒè·¯å¾„æ­£åˆ™åŒ–æŸå¤±ã€‚
            # # è®¡ç®—bpræŸå¤±
            mf_loss_1 = self.create_bpr_loss(self.u_g_embeddings_1, self.pos_i_g_embeddings_1, self.neg_i_g_embeddings_1)
            mf_loss_2 = self.create_bpr_loss(self.u_g_embeddings_2, self.pos_i_g_embeddings_2, self.neg_i_g_embeddings_2)
            # è®¡ç®—SSLæŸå¤±
            ssl_loss = self.calc_ssl_loss_strategy(self.layer_embeddings_1, self.layer_embeddings_2, self.n_users_1, self.n_users_2, self.n_items_1, self.n_items_2)
            # åŸºäºBPRçš„ MFï¼ˆMatrix Factorizationï¼‰æŸå¤± + åµŒå…¥é¡¹çš„æ­£åˆ™åŒ–æŸå¤± ï¼šSSLæŸå¤±å‡½æ•°ï¼ˆé¢å¤–ä¹˜ä»¥ä¸€ä¸ªæƒé‡å¸¸å¸¸ç”¨æ¥å¹³è¡¡ä¸åŒæŸå¤±é¡¹çš„è´¡çŒ®ï¼‰
            variationloss = mf_loss_1 + mf_loss_2 + self.alpha * ssl_loss
            # Regularization: the constraint of transformed reasonableness

            metaregloss_1 = self.calculate_metaregloss(self.ui_userEmbedding_1[userindex_S.cpu().numpy()], self.userEmbedding_1, self.ui_itemEmbedding_1[itemindex_S.cpu().numpy()],
                                                       self.itemEmbedding_1, self.uuMat_1[userindex_S.cpu().numpy()], self.iiMat_1[itemindex_S.cpu().numpy()])
            metaregloss_2 = self.calculate_metaregloss(self.ui_userEmbedding_2[userindex_T.cpu().numpy()], self.userEmbedding_2, self.ui_itemEmbedding_2[itemindex_T.cpu().numpy()],
                                                       self.itemEmbedding_2, self.uuMat_2[userindex_T.cpu().numpy()], self.iiMat_2[itemindex_T.cpu().numpy()])
            # Contrastive Learning of collaborative relations
            ssl_loss_user_S = self.ssl_loss(self.ui_userEmbedding_1, self.userEmbedding_1, users_1)
            ssl_loss_user_T = self.ssl_loss(self.ui_userEmbedding_2, self.userEmbedding_2, users_2)
            ssl_loss_item_S = self.ssl_loss(self.ui_itemEmbedding_1, self.itemEmbedding_1, pos_items_1)
            ssl_loss_item_T = self.ssl_loss(self.ui_itemEmbedding_2, self.itemEmbedding_2, pos_items_2)
            ssl_loss_S = self.ssl_ureg * ssl_loss_user_S + self.ssl_ireg * ssl_loss_item_S
            ssl_loss_T = self.ssl_ureg * ssl_loss_user_T + self.ssl_ireg * ssl_loss_item_T

            # è®¡ç®—regloss
            regLoss_S = (t.norm(self.fu1[users_1]) ** 2 + t.norm(self.fi1[pos_items_1]) ** 2 + t.norm(
                self.fi1[neg_items_1]) ** 2)
            regLoss_T = (t.norm(self.fu2[users_1]) ** 2 + t.norm(self.fi2[pos_items_1]) ** 2 + t.norm(
                self.fi2[neg_items_1]) ** 2)

        trainneed = {}
        trainneed['metaregloss_1'] = metaregloss_1
        trainneed['metaregloss_2'] = metaregloss_2
        trainneed['ssl_loss_S'] = ssl_loss_S
        trainneed['ssl_loss_T'] = ssl_loss_T
        trainneed['variationloss'] = variationloss
        trainneed['regLoss_S'] = regLoss_S
        trainneed['regLoss_T'] = regLoss_T
        return trainneed


    def get_embeddings(self, users_1, pos_items_1, neg_items_1, users_2, pos_items_2, neg_items_2):
        self.u_g_embeddings_1 = t.index_select(self.fu1, 0, t.LongTensor(users_1).to(self.device))
        self.pos_i_g_embeddings_1 = t.index_select(self.fi1, 0, t.LongTensor(pos_items_1).to(self.device))
        self.neg_i_g_embeddings_1 = t.index_select(self.fi1, 0, t.LongTensor(neg_items_1).to(self.device))

        self.u_g_embeddings_2 = t.index_select(self.fu2, 0, t.LongTensor(users_2).to(self.device))
        self.pos_i_g_embeddings_2 = t.index_select(self.fi2, 0, t.LongTensor(pos_items_2).to(self.device))
        self.neg_i_g_embeddings_2 = t.index_select(self.fi2, 0, t.LongTensor(neg_items_2).to(self.device))

class GCN_layer(nn.Module):
    def __init__(self):
        super(GCN_layer, self).__init__()

    # è¿™ä¸ªæ–¹æ³•ç”¨äºå°†ä¸€ä¸ª scipy ç¨€ç–çŸ©é˜µè½¬æ¢ä¸º PyTorch çš„ç¨€ç–å¼ é‡
    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):
        """Convert a scipy sparse matrix to a torch sparse tensor."""
        if type(sparse_mx) != sp.coo_matrix:
            sparse_mx = sparse_mx.tocoo().astype(np.float32)
        indices = t.from_numpy(
            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))
        values = t.from_numpy(sparse_mx.data).float()
        shape = t.Size(sparse_mx.shape)
        return t.sparse.FloatTensor(indices, values, shape)

    # è¿™ä¸ªæ–¹æ³•ç”¨äºå¯¹è¾“å…¥çš„é‚»æ¥çŸ©é˜µè¿›è¡Œå¯¹ç§°å½’ä¸€åŒ–
    def normalize_adj(self, adj):
        """Symmetrically normalize adjacency matrix."""
        adj = sp.coo_matrix(adj)
        rowsum = np.array(adj.sum(1))
        rowsum = np.maximum(rowsum, 1e-12)
        d_inv_sqrt = np.power(rowsum, -0.5).flatten()
        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)
        return (d_mat_inv_sqrt).dot(adj).dot(d_mat_inv_sqrt).tocoo()

    # è¿™ä¸ªæ–¹æ³•æ˜¯ GCN_layer çš„å‰å‘ä¼ æ’­å‡½æ•°ï¼Œç”¨äºå¯¹è¾“å…¥çš„ç‰¹å¾æ•°æ®è¿›è¡Œ GCN å·ç§¯æ“ä½œ
    # å®ƒæ¥å—å›¾çš„ é‚»æ¥çŸ©é˜µ å’Œ èŠ‚ç‚¹ç‰¹å¾ ï¼Œå¹¶æ ¹æ® é‚»æ¥å…³ç³» è¿›è¡Œå·ç§¯æ“ä½œï¼Œç„¶åæ›´æ–°æŒ‡å®šèŠ‚ç‚¹çš„ç‰¹å¾ã€‚
    def forward(self, features, Mat, index):
        subset_Mat = Mat
        subset_features = features
        subset_Mat = self.normalize_adj(subset_Mat)  # å°†é‚»æ¥çŸ©é˜µå¯¹ç§°å½’ä¸€åŒ–
        subset_sparse_tensor = self.sparse_mx_to_torch_sparse_tensor(subset_Mat).cuda()   # # è¿™ä¸ªè¦è®°å¾—æ”¹æˆcuda

        start_time = timeit.default_timer()
        out_features = t.spmm(subset_sparse_tensor, subset_features).cuda()
        end_time = timeit.default_timer()
        # print("class GCN_layer in t.spmm:", end_time - start_time)

        new_features = t.empty(features.shape).cuda()    # # è¿™ä¸ªè¦è®°å¾—æ”¹æˆcuda
        new_features[index] = out_features
        dif_index = np.setdiff1d(t.arange(features.shape[0]), index)
        new_features[dif_index] = features[dif_index]
        return new_features


class MLP(t.nn.Module):
    # input_dimï¼šè¾“å…¥ç‰¹å¾çš„ç»´åº¦ã€‚
    # feature_dimï¼šä¸­é—´ç‰¹å¾çš„ç»´åº¦ï¼ˆå¦‚æœ feature_pre ä¸º Trueï¼Œåˆ™è¡¨ç¤ºä¸­é—´å±‚çš„ç»´åº¦ï¼‰ã€‚
    # hidden_dimï¼šä¸­é—´å±‚çš„ç»´åº¦ï¼ˆå¦‚æœ feature_pre ä¸º Falseï¼Œåˆ™è¡¨ç¤ºä¸­é—´å±‚çš„ç»´åº¦ï¼‰ã€‚
    # output_dimï¼šè¾“å‡ºå±‚çš„ç»´åº¦ã€‚
    # feature_preï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦åœ¨ç¬¬ä¸€å±‚åº”ç”¨çº¿æ€§å˜æ¢åˆ° feature_dim ç»´åº¦ã€‚
    # layer_numï¼šMLP çš„å±‚æ•°ï¼Œé»˜è®¤ä¸º 2ã€‚
    # dropoutï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦åœ¨ä¸­é—´å±‚åº”ç”¨ dropout
    def __init__(self, input_dim, feature_dim, hidden_dim, output_dim,
                 feature_pre=True, layer_num=2, dropout=True, **kwargs):
        super(MLP, self).__init__()
        self.feature_pre = feature_pre
        self.layer_num = layer_num
        self.dropout = dropout
        if feature_pre:
            self.linear_pre =   nn.Linear(input_dim, feature_dim,bias=True)
        else:
            self.linear_first = nn.Linear(input_dim, hidden_dim)
        self.linear_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for i in range(layer_num - 2)])
        self.linear_out = nn.Linear(feature_dim, output_dim,bias=True)

    def forward(self, data):
        x = data
        if self.feature_pre:
            x = self.linear_pre(x)
        prelu=nn.PReLU().cuda()   # # è¿™ä¸ªè¦è®°å¾—æ”¹æˆcuda
        x = prelu(x)
        for i in range(self.layer_num - 2):
            x = self.linear_hidden[i](x)
            x = F.tanh(x)
            if self.dropout:
                x = F.dropout(x, training=self.training)
        x = self.linear_out(x)
        x = F.normalize(x, p=2, dim=-1)
        return x